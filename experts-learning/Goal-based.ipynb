{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tnrange\n",
    "from random import choice\n",
    "\n",
    "from gridworld import GridworldEnv\n",
    "from nets import *\n",
    "from draw_methods import *\n",
    "from hyper_parametrs import *\n",
    "from env_methods import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logs_directory)\n",
    "board = command.Command('tensorboard --logdir=run1:{} --port {}'.format(logs_directory, board_port))\n",
    "board.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/anaconda/lib/python3.5/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['choice']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = GridworldEnv(shape=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dim = int(env.nS * 2)\n",
    "action_dim = int(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_term(probs):\n",
    "    return -torch.sum(probs * torch.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params(agent, optimizer, losses_history):\n",
    "    R = 0\n",
    "    saved_actions = agent.saved_actions\n",
    "    value_loss = 0\n",
    "    rewards = []\n",
    "    for r in agent.rewards[::-1]:\n",
    "        R = r + gamma_rl * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    log_loss = Variable(torch.FloatTensor([0]))\n",
    "    val_loss = Variable(torch.FloatTensor([0]))\n",
    "    entropy_loss = Variable(torch.FloatTensor([0]))\n",
    "    for (action, probs, value), r in zip(saved_actions, rewards):\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        reward = r - value.data[0, 0]\n",
    "        log_loss  += -(m.log_prob(action[0]) * reward)\n",
    "        val_loss += lambda_baseline * F.mse_loss(value, Variable(torch.Tensor([r])))\n",
    "        entropy_loss += -entropy_weights[\"agent\"] * entropy_term(probs)\n",
    "    \n",
    "    losses_history[\"entropy\"].append(entropy_loss.data.numpy()[0])\n",
    "    losses_history[\"value loss\"].append(val_loss.data.numpy()[0])\n",
    "    losses_history[\"log loss\"].append(log_loss.data.numpy()[0])\n",
    "    loss = log_loss + val_loss + entropy_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del agent.rewards[:]\n",
    "    del agent.saved_actions[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Expert_left_corner = Expert(\"left_top_corner\")\n",
    "Expert_left_corner.precompute_v_function(env)\n",
    "Expert_right_corner = Expert(\"right_bottom_corner\")\n",
    "Expert_right_corner.precompute_v_function(env)\n",
    "Expert_global_optimal = Expert(\"global_optimal\")\n",
    "Expert_global_optimal.precompute_v_function(env)\n",
    "possible_s_stars = [0, env.nS - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def goal_unsim(env, g1, g2):\n",
    "    return abs(g1 % int(np.sqrt(env.nS)) - g2 % int(np.sqrt(env.nS))) + abs(g1 // int(np.sqrt(env.nS)) - g2 // int(np.sqrt(env.nS)))\n",
    "\n",
    "def goal_based_training(env, tmax, agent, goal, optimizer, losses_history, agent_type): \n",
    "    time = 0\n",
    "    done = False\n",
    "    while True:\n",
    "        if is_terminal(env, env.s): #done\n",
    "            break\n",
    "        time += 1\n",
    "        a = agent.select_action(env.s, goal)\n",
    "        state, _, done, _= env.step(a[0, 0])\n",
    "        if is_terminal(env, env.s):\n",
    "            if is_terminal(env, env.s): #done\n",
    "                break\n",
    "        if time >= tmax:\n",
    "            break\n",
    "        agent.rewards.append(0)\n",
    "    \n",
    "    if agent_type == \"goal-based\":\n",
    "        final_reward = (-time - lambda_goals  * goal_unsim(env, goal, env.s)) / scale_reward \n",
    "    else:\n",
    "        final_reward = (-time) / scale_reward \n",
    "    agent.rewards.append(final_reward)\n",
    "    if(len(agent.saved_actions)):\n",
    "        update_params(agent, optimizer, losses_history)\n",
    "    \n",
    "    del agent.rewards[:]\n",
    "    del agent.saved_actions[:]\n",
    "\n",
    "    return final_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def choose_goal(env, s0, agent_type=\"goal-based\"):\n",
    "        if agent_type == \"goal-based\":\n",
    "            if (s0 // np.sqrt(env.nS) <= int(np.sqrt(env.nS)) - (s0 % int(np.sqrt(env.nS))) - 1):\n",
    "                return 0\n",
    "            if (s0 // np.sqrt(env.nS) > int(np.sqrt(env.nS)) - (s0 % int(np.sqrt(env.nS))) - 1):\n",
    "                return env.nS - 1\n",
    "            #else:\n",
    "            #    return np.random.choice([0, env.nS - 1])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "def train_agent(env, tmax, each_goal_times, agent, optimizer, losses_history, agent_type):\n",
    "    rewards = []\n",
    "    episode_time = []\n",
    "    finish_states = []\n",
    "    set_random_s0(env)\n",
    "    s0 = env.reset()\n",
    "    goal = choose_goal(env, s0, agent_type)\n",
    "    res = goal_based_training(env, tmax, agent, goal, optimizer, losses_history, agent_type)   \n",
    "    rewards.append(res)\n",
    "    \n",
    "    return agent, np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_n_episodes(n, env, model, s_star, tmax=40):\n",
    "    state_dist_list = []\n",
    "    for i in range(n):\n",
    "        set_random_s0(env)\n",
    "        env.reset()\n",
    "        state_dist_episode = np.zeros(env.shape)\n",
    "        for j in range(tmax):\n",
    "            s = env.s\n",
    "            state_dist_episode[s // env.shape[0]][s % env.shape[1]] += 1\n",
    "            if(is_terminal(env, env.s)):\n",
    "                break\n",
    "            probs, state_value = model(s, s_star)\n",
    "            action = probs.multinomial().data\n",
    "            env.step(action[0][0])\n",
    "        state_dist_list.append(state_dist_episode)\n",
    "    \n",
    "    state_dist = np.zeros(env.shape)\n",
    "    \n",
    "    for dist in state_dist_list:\n",
    "        state_dist += dist\n",
    "    \n",
    "    return state_dist / n, state_dist_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_goal_based = Agent(state_dim, action_dim, \"goal-based\")\n",
    "agent_simple = Agent(state_dim, action_dim, \"simple\")\n",
    "\n",
    "optimizer_goal_based_agent = optim.RMSprop(agent_goal_based.parameters(), alpha=0.97, eps=1e-6, lr=lr_agent)\n",
    "optimizer_simple_agent = optim.RMSprop(agent_simple.parameters(), alpha=0.97, eps=1e-6, lr=lr_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "plots_dir = \"plots_compare\"\n",
    "try:\n",
    "    shutil.rmtree(plots_dir)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(plots_dir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns;\n",
    "# sns.set(color_codes=True)\n",
    "# fig, ax = subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "# s_stars = [0, env.nS - 1, None]\n",
    "# colors = sns.color_palette(\"Set1\", n_colors=3, desat=.75)\n",
    "# for s in range(1, env.nS - 1):\n",
    "#     ax.set_xticks(np.arange(0, 6))\n",
    "#     ax.set_yticks(np.arange(0, 6))\n",
    "#     probs = Expert_global_optimal.get_action_probs(env, s)\n",
    "#     draw_direction_probs(ax, env, s, probs)\n",
    "#     ax.invert_yaxis()\n",
    "\n",
    "# fig.tight_layout()\n",
    "# #img = fig2img(fig)\n",
    "# #writer.add_image('Action_Probs', np.array(img)[..., :3], 1)\n",
    "\n",
    "\n",
    "\n",
    "# #plt.savefig(os.path.join(plots_dir, \"action_prob_plot{}\".format(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns;\n",
    "# sns.set(color_codes=True)\n",
    "# fig, ax = subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "# s_stars = [0, env.nS - 1, None]\n",
    "# colors = sns.color_palette(\"Set1\", n_colors=3, desat=.75)\n",
    "# for j, s_star in enumerate(s_stars):\n",
    "#     for s in range(1, env.nS - 1):\n",
    "#         if s is None:\n",
    "#             agent = agent_simle\n",
    "#         else:\n",
    "#             agent = agent_goal_based\n",
    "#         ax[j].set_xticks(np.arange(0, 6))\n",
    "#         ax[j].set_yticks(np.arange(0, 6))\n",
    "#         probs, state_value = agent(s, s_star)\n",
    "#         draw_direction_probs(ax[j], env, s, probs[0].data, arrow_color=colors[j])\n",
    "#         ax[j].set_title(\"S_star={}\".format(str(s_star)), fontsize=16)\n",
    "#         ax[j].invert_yaxis()\n",
    "\n",
    "# fig.tight_layout()\n",
    "# img = fig2img(fig)\n",
    "# writer.add_image('Action_Probs', np.array(img)[..., :3], 1)\n",
    "\n",
    "\n",
    "\n",
    "# #plt.savefig(os.path.join(plots_dir, \"action_prob_plot{}\".format(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses_history_goal_based = {\n",
    "    \"entropy\":[],\n",
    "    \"value loss\":[],\n",
    "    \"log loss\":[]\n",
    "}\n",
    "\n",
    "losses_history_simple = {\n",
    "    \"entropy\":[],\n",
    "    \"value loss\":[],\n",
    "    \"log loss\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    }
   ],
   "source": [
    "each_goal_times = 1\n",
    "times = 2000\n",
    "start = 0\n",
    "\n",
    "models = [agent_goal_based, agent_simple, Expert_global_optimal]\n",
    "previous_rewards = [[] for i in range(len(models))]\n",
    "\n",
    "for i in tnrange(times):\n",
    "    agent_goal_based, rewards = train_agent(env, tmax, each_goal_times, agent_goal_based, optimizer_goal_based_agent, losses_history_goal_based, agent_type=\"goal-based\")\n",
    "    agent_simple, rewards = train_agent(env, tmax, each_goal_times, agent_simple, optimizer_simple_agent, losses_history_simple, agent_type=\"simple\")\n",
    "    \n",
    "    fig, ax = subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    models = [agent_goal_based, agent_simple, Expert_global_optimal]\n",
    "    models_anotations = [\"goal-based agent\", \"simple agent\", \"Expert global optimal\"]\n",
    "    previous_rewards = draw_reward_curves(writer, i, ax, env, models, models_anotations, tmax, previous_rewards, possible_s_stars)\n",
    "    #fig.tight_layout()\n",
    "    #plt.savefig(os.path.join(plots_dir, \"rewards_curves{}\".format(each_goal_times * (start + i + 1))))\n",
    "    #plt.cla()\n",
    "    #plt.clf()\n",
    "    \n",
    "    \n",
    "    #actions_prob_plot\n",
    "    sns.set(color_codes=True)\n",
    "    fig, ax = subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "    s_stars = [0, env.nS - 1, None]\n",
    "    colors = sns.color_palette(\"Set1\", n_colors=3, desat=.75)\n",
    "    for j, s_star in enumerate(s_stars):\n",
    "        for s in range(1, env.nS - 1):\n",
    "            if s is None:\n",
    "                agent = agent_simle\n",
    "            else:\n",
    "                agent = agent_goal_based\n",
    "            ax[j].set_xticks(np.arange(0, 6))\n",
    "            ax[j].set_yticks(np.arange(0, 6))\n",
    "            probs, state_value = agent(s, s_star)\n",
    "            draw_direction_probs(ax[j], env, s, probs[0].data, arrow_color=colors[j])\n",
    "            if s is None:\n",
    "                ax[j].set_title(\"Simple agent\", fontsize=16)\n",
    "            else:\n",
    "                ax[j].set_title(\"Goal-based agent, s_star={}\".format(str(s_star)), fontsize=16)\n",
    "            ax[j].invert_yaxis()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    img = fig2img(fig)\n",
    "    writer.add_image('Action_Probs', np.array(img)[..., :3], i)\n",
    "    #plt.savefig(os.path.join(plots_dir, \"action_prob_plot{}\".format(each_goal_times * (start + i + 1))))\n",
    "    #plt.cla()\n",
    "    #plt.clf()\n",
    "    \n",
    "#     fig, ax = subplots(nrows=1, ncols=1, figsize=(15, 5))\n",
    "#     ax.plot(losses_history_goal_based[\"entropy\"][-100:], label=\"entropy term\")\n",
    "#     ax.plot(losses_history_goal_based[\"value loss\"][-100:], label=\"value term\")\n",
    "#     ax.plot(losses_history_goal_based[\"log loss\"][-100:], label=\"log loss term\")\n",
    "#     ax.legend(loc=\"best\")\n",
    "#     ax.set_xlabel(\"Episode\")\n",
    "#     ax.set_ylabel(\"Loss\")\n",
    "#     fig.tight_layout()\n",
    "#     img = fig2img(fig)\n",
    "    writer.add_scalars('loss terms',\n",
    "                       {\n",
    "                            \"entropy term\": losses_history_goal_based[\"entropy\"][-1],\n",
    "                            \"value term\": losses_history_goal_based[\"value loss\"][-1],\n",
    "                            \"log loss term\": losses_history_goal_based[\"log loss\"][-1],\n",
    "                       }, \n",
    "                       i)\n",
    "    #plt.savefig(os.path.join(plots_dir, \"loss terms\"))\n",
    "    #plt.cla()\n",
    "    #plt.clf()\n",
    "    \n",
    "#     #reward_time_plot\n",
    "#     fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "#     ax[0].plot(rewards, label=\"Bob's reward\")\n",
    "#     ax[1].plot(time, label=\"Bob's time\")\n",
    "#     ax[2].hist(finish_states, color='c', alpha=0.5, label=\"Bob\")\n",
    "\n",
    "#     ax[0].set_xlabel(\"episode\")\n",
    "#     ax[1].set_xlabel(\"episode\")\n",
    "#     ax[1].set_ylabel(\"steps\")\n",
    "#     ax[0].set_ylabel(\"reward\")\n",
    "#     ax[2].set_xlabel(\"final state\")\n",
    "#     ax[2].set_ylabel(\"distribution\")\n",
    "#     ax[0].legend(loc=\"best\")\n",
    "#     ax[1].legend(loc=\"best\")\n",
    "#     ax[2].legend(loc=\"best\")\n",
    "#     fig.tight_layout()\n",
    "#     plt.savefig(os.path.join(plots_dir, \"reward_time_final{}\".format(episodes_step * (start + i + 1))))\n",
    "#     plt.clf()\n",
    "#     #plt.savefig(\"plots_union_no_entropy_one_hot/reward_time_final{}\".format(episodes_step * (start + i + 1)))\n",
    "    \n",
    "#     #final_states_heat_map\n",
    "#     dense_Bob = np.zeros((5, 5))\n",
    "#     for finish in finish_states:\n",
    "#         dense_Bob[finish // 5][finish % 5] += 1\n",
    "\n",
    "#     fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "\n",
    "#     ax.imshow(dense_Bob, cmap='hot', interpolation='nearest')\n",
    "#     ax.set_title(\"Bob final states\")\n",
    "\n",
    "#     plt.savefig(os.path.join(plots_dir, \"final_states{}\".format(episodes_step * (start + i + 1))))\n",
    "#     plt.clf()\n",
    "    \n",
    "    fig, ax = subplots(nrows=1, ncols=3, figsize=(12, 5))\n",
    "    for j in range(3):\n",
    "        ax[j].set_xticks(np.arange(0, 6))\n",
    "        ax[j].set_yticks(np.arange(0, 6))\n",
    "        ax[j].invert_yaxis()\n",
    "\n",
    "    for j, s_star in enumerate([0, 24, None]):\n",
    "        if s_star is None:\n",
    "            agent = agent_simple\n",
    "        else:\n",
    "            agent = agent_goal_based\n",
    "        #print(agent.name)\n",
    "        state_dist, state_dist_list = play_n_episodes(10, env, agent, s_star)   \n",
    "        for s in range(1, env.nS - 1):\n",
    "            value = agent(s, s_star)[1].data[0][0]\n",
    "            #states_heat_map[int(s // np.sqrt(env.nS))][s % int(np.sqrt(env.nS))] = value\n",
    "            draw_value_anotate(ax[j], env, s, value)\n",
    "        ax[j].imshow(state_dist, cmap='hot', interpolation='nearest')\n",
    "        if s is None:\n",
    "            ax[j].set_title(\"Simple agent\", fontsilze=16)\n",
    "        else:\n",
    "            ax[j].set_title(\"Goal-based agent, s_star={}\".format(str(s_star)), fontsize=16)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    img = fig2img(fig)\n",
    "    writer.add_image('Value state distr', np.array(img)[..., :3], i)\n",
    "    \n",
    "    #plt.savefig(os.path.join(plots_dir, \"value_state_distr{}\".format(each_goal_times * (start + i + 1))))\n",
    "    #plt.cla()\n",
    "    #plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board.wait(board_timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(get_policy_reward_estimation(env, agent_goal_based, 'agent', 1000, 40, s_star=[0, 24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 1\n",
    "agent_goal_based(s, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_simple(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(get_policy_reward_estimation(env, agent_simple, 'agent', 1000, 40, s_star=[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_simple(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = Variable(torch.FloatTensor([0.5, 0.5]))\n",
    "m = torch.distributions.Categorical(probs)\n",
    "m.log_prob(Variable(torch.LongTensor(1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# import seaborn as sns\n",
    "# fig, ax = subplots(nrows=1, ncols=3, figsize=(12, 5))\n",
    "# for i in range(3):\n",
    "#     ax[i].set_xticks(np.arange(0, 6))\n",
    "#     ax[i].set_yticks(np.arange(0, 6))\n",
    "#     ax[i].invert_yaxis()\n",
    "\n",
    "# for i, s_star in enumerate([0, 24, None]):\n",
    "#     state_dist, state_dist_list = play_n_episodes(100, env, model_Bob, s_star)   \n",
    "#     for s in range(env.nS):\n",
    "#         value = model_Bob(s, s_star)[1].data[0][0]\n",
    "#         #states_heat_map[int(s // np.sqrt(env.nS))][s % int(np.sqrt(env.nS))] = value\n",
    "#         draw_value_anotate(ax[i], env, s, value)\n",
    "#     ax[i].imshow(state_dist, cmap='hot', interpolation='nearest')\n",
    "# #ax[0].colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "f775d956dfdb4e4b8384c1007f6aa31f": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

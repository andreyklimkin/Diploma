{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import shutil\n",
    "\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tnrange\n",
    "from random import choice\n",
    "\n",
    "from complex_gridword import GridworldEnv\n",
    "from nets import *\n",
    "from draw_methods import *\n",
    "from hyper_parametrs import *\n",
    "from env_methods import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shutil.rmtree(logs_directory)\n",
    "writer = SummaryWriter(logs_directory)\n",
    "board = command.Command('tensorboard --logdir=run1:{} --port {}'.format(logs_directory, board_port))\n",
    "board.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shutil.copy(\"hyper_parametrs.py\", logs_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/anaconda/lib/python3.5/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['choice']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_size = (5, 5)\n",
    "env_name = \"simple\"\n",
    "grid_map = np.load(\"gridworlds_data/{}_{}x{}/grid_map.npy\".format(env_name, grid_size[0], grid_size[1]))\n",
    "env = GridworldEnv(grid_size, grid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dim = int(env.nS * 2)\n",
    "action_dim = int(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_term(probs):\n",
    "    return -torch.sum(probs * torch.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params(agent, optimizer, losses_history):\n",
    "    R = 0\n",
    "    saved_actions = agent.saved_actions\n",
    "    value_loss = 0\n",
    "    rewards = []\n",
    "    for r in agent.rewards[::-1]:\n",
    "        R = r + gamma_rl * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    log_loss = Variable(torch.FloatTensor([0]))\n",
    "    val_loss = Variable(torch.FloatTensor([0]))\n",
    "    entropy_loss = Variable(torch.FloatTensor([0]))\n",
    "    for (action, probs, value), r in zip(saved_actions, rewards):\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        reward = r - value.data[0, 0]\n",
    "        log_loss  += -(m.log_prob(action[0]) * reward)\n",
    "        val_loss += lambda_baseline * F.mse_loss(value, Variable(torch.Tensor([r])))\n",
    "        entropy_loss += -entropy_weights[\"agent\"] * entropy_term(probs)\n",
    "    \n",
    "    losses_history[\"entropy\"].append(entropy_loss.data.numpy()[0])\n",
    "    losses_history[\"value loss\"].append(val_loss.data.numpy()[0])\n",
    "    losses_history[\"log loss\"].append(log_loss.data.numpy()[0])\n",
    "    loss = log_loss + val_loss + entropy_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del agent.rewards[:]\n",
    "    del agent.saved_actions[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Expert_left_corner = NovelExpert(\"left_top_corner\", np.load(\"gridworlds_data/{}_{}x{}/agents_left_corner/action_probs.npy\".format(env_name, grid_size[0], grid_size[1])), \n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_left_corner/value_function.npy\".format(env_name, grid_size[0], grid_size[1])),\n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_left_corner/goal_map.npy\".format(env_name, grid_size[0], grid_size[1])))\n",
    "Expert_right_corner = NovelExpert(\"right_bottom_corner\", np.load(\"gridworlds_data/{}_{}x{}/agents_right_corner/action_probs.npy\".format(env_name, grid_size[0], grid_size[1])), \n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_right_corner/value_function.npy\".format(env_name, grid_size[0], grid_size[1])),\n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_right_corner/goal_map.npy\".format(env_name, grid_size[0], grid_size[1])))\n",
    "Expert_global_optimal = NovelExpert(\"global_optimal\", np.load(\"gridworlds_data/{}_{}x{}/agents_global_optimal/action_probs.npy\".format(env_name, grid_size[0], grid_size[1])), \n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_global_optimal/value_function.npy\".format(env_name, grid_size[0], grid_size[1])),\n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_global_optimal/goal_map.npy\".format(env_name, grid_size[0], grid_size[1])))\n",
    "possible_s_stars = [0, env.nS - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siamese_net = torch.load(\"pretrained_nets/siamese_{}_{}x{}.pt\".format(env_name, grid_size[0], grid_size[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def goal_unsim(env, g1, g2):\n",
    "    #print(g1)\n",
    "    #print(g2)\n",
    "    return (1.0 - siamese_net(np.array([(g1, g2)]))).data.numpy()[0][0]\n",
    "   #return abs(g1 % int(np.sqrt(env.nS)) - g2 % int(np.sqrt(env.nS))) + abs(g1 // int(np.sqrt(env.nS)) - g2 // int(np.sqrt(env.nS)))\n",
    "\n",
    "# def goal_unsim(env, g1, g2, env_type=\"pillar\"):\n",
    "#     if env_type == \"simple\":\n",
    "#         return abs(g1 % int(np.sqrt(env.nS)) - g2 % int(np.sqrt(env.nS))) + abs(g1 // int(np.sqrt(env.nS)) - g2 // int(np.sqrt(env.nS)))\n",
    "#     else:\n",
    "#         if env.grid_map[g1] == 0 or env.grid_map[g2] == 0:\n",
    "#             return 1000\n",
    "#         r1 = np.abs(Expert_left_corner.v_function[g1] - Expert_left_corner.v_function[g2])\n",
    "#         r2 = np.abs(Expert_right_corner.v_function[g1] - Expert_right_corner.v_function[g2])\n",
    "#         return min(r1, r2)\n",
    "    \n",
    "def goal_based_training(env, tmax, agent, goal, optimizer, losses_history, agent_type): \n",
    "    time = 0\n",
    "    done = False\n",
    "    while True:\n",
    "        if is_terminal(env, env.s): #done\n",
    "            break\n",
    "        time += 1\n",
    "        a = agent.select_action(env.s, goal)\n",
    "        state, _, done, _= env.step(a[0, 0])\n",
    "        if is_terminal(env, env.s):\n",
    "            if is_terminal(env, env.s): #done\n",
    "                break\n",
    "        if time >= tmax:\n",
    "            break\n",
    "        agent.rewards.append(0)\n",
    "    \n",
    "    if agent_type == \"goal-based\":\n",
    "        final_reward = (-time - lambda_goals  * goal_unsim(env, goal, env.s)) / scale_reward \n",
    "    else:\n",
    "        final_reward = (-time) / scale_reward \n",
    "    agent.rewards.append(final_reward)\n",
    "    if(len(agent.saved_actions)):\n",
    "        update_params(agent, optimizer, losses_history)\n",
    "    \n",
    "    del agent.rewards[:]\n",
    "    del agent.saved_actions[:]\n",
    "\n",
    "    return final_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def choose_goal(env, experts, s0, iteration, agent_type=\"goal-based\"):\n",
    "    #print(s0)\n",
    "    if agent_type == \"goal-based\":\n",
    "        best_goal = None\n",
    "        best_goal_v = -np.inf\n",
    "        possible_goals = [0, env.nS - 1]\n",
    "        for expert in experts:\n",
    "            possible_goals.append(expert.get_goal(s0))\n",
    "            if expert.v_function[s0] > best_goal_v:\n",
    "                best_goal = expert.get_goal(s0)\n",
    "                best_goal_v = expert.v_function[s0]\n",
    "        \n",
    "        if goal_sampling_strategy == \"eps_greedy\":\n",
    "            if np.random.random() < goal_eps:\n",
    "                goal_eps *= goal_eps_decay\n",
    "                return np.random.choice(possible_goals) \n",
    "            else:\n",
    "                return int(best_goal)\n",
    "        \n",
    "        if goal_sampling_strategy == \"random\":\n",
    "            return int(possible_goals[iteration % 2])\n",
    "    return None\n",
    "    \n",
    "def train_agent(env, tmax, each_goal_times, agent, experts, optimizer, losses_history, i, agent_type):\n",
    "    rewards = []\n",
    "    episode_time = []\n",
    "    finish_states = []\n",
    "    set_random_s0(env)\n",
    "    s0 = env.reset()\n",
    "    goal = choose_goal(env, experts, s0, i, agent_type)\n",
    "    res = goal_based_training(env, tmax, agent, goal, optimizer, losses_history, agent_type)   \n",
    "    rewards.append(res)\n",
    "    \n",
    "    return agent, np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_env_s0(env, s0):\n",
    "    env.isd = np.zeros(env.shape[0] * env.shape[1])\n",
    "    env.isd[s0] = 1 \n",
    "\n",
    "def play_n_episodes(n, env, model, s0_list, s_star, tmax=40):\n",
    "    state_dist_list = []\n",
    "    for i in range(n):\n",
    "        set_env_s0(env, s0_list[i])\n",
    "        env.reset()\n",
    "        state_dist_episode = np.zeros(env.shape)\n",
    "        for j in range(tmax):\n",
    "            s = env.s\n",
    "            state_dist_episode[s // env.shape[0]][s % env.shape[1]] += 1\n",
    "            if(is_terminal(env, env.s)):\n",
    "                break\n",
    "            probs, state_value = model(s, s_star)\n",
    "            action = probs.multinomial().data\n",
    "            env.step(action[0][0])\n",
    "        state_dist_list.append(state_dist_episode)\n",
    "    \n",
    "    state_dist = np.zeros(env.shape)\n",
    "    \n",
    "    for dist in state_dist_list:\n",
    "        state_dist += dist\n",
    "    \n",
    "    return state_dist / n, state_dist_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "agent_goal_based = Agent(state_dim, action_dim, \"goal-based\")\n",
    "agent_simple = Agent(state_dim, action_dim, \"simple\")\n",
    "\n",
    "#optimizer_goal_based_agent = optim.RMSprop(agent_goal_based.parameters(), alpha=0.97, eps=1e-6, lr=lr_agent)\n",
    "#optimizer_simple_agent = optim.RMSprop(agent_simple.parameters(), alpha=0.97, eps=1e-6, lr=lr_agent)\n",
    "\n",
    "optimizer_goal_based_agent = optim.Adam(agent_goal_based.parameters(),lr=lr_agent, weight_decay=weight_decay)\n",
    "optimizer_simple_agent = optim.Adam(agent_simple.parameters(),lr=lr_agent, weight_decay=weight_decay)\n",
    "\n",
    "goal_based_lr_scheduler = lr_scheduler.StepLR(optimizer_goal_based_agent, step_size=decrease_lr_every, gamma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "plots_dir = \"plots_compare\"\n",
    "try:\n",
    "    shutil.rmtree(plots_dir)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(plots_dir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses_history_goal_based = {\n",
    "    \"entropy\":[],\n",
    "    \"value loss\":[],\n",
    "    \"log loss\":[]\n",
    "}\n",
    "\n",
    "losses_history_simple = {\n",
    "    \"entropy\":[],\n",
    "    \"value loss\":[],\n",
    "    \"log loss\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    }
   ],
   "source": [
    "each_goal_times = 1\n",
    "start = 0\n",
    "\n",
    "models = [agent_goal_based, agent_simple, Expert_global_optimal]\n",
    "experts = [Expert_global_optimal]\n",
    "previous_rewards = [[] for i in range(len(models))]\n",
    "\n",
    "draw_every = 250\n",
    "estimation_episodes_num = 500\n",
    "\n",
    "for i in tnrange(train_steps):\n",
    "    goal_based_lr_scheduler.step()\n",
    "    agent_goal_based, rewards = train_agent(env, tmax, each_goal_times, agent_goal_based, experts, optimizer_goal_based_agent, losses_history_goal_based, i, agent_type=\"goal-based\")\n",
    "    agent_simple, rewards = train_agent(env, tmax, each_goal_times, agent_simple, experts, optimizer_simple_agent, losses_history_simple, i, agent_type=\"simple\")\n",
    "    \n",
    "    #fig, ax = subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "    models = [agent_goal_based, agent_simple, Expert_global_optimal]\n",
    "    models_anotations = [\"goal-based agent\", \"simple agent\", \"Expert global optimal\"]\n",
    "    \n",
    "    \n",
    "    if (i + 1) % draw_every == 0:\n",
    "        \n",
    "        s0_list = np.random.choice(np.array(np.where(env.grid_map)).flatten(), size=estimation_episodes_num)\n",
    "        \n",
    "        draw_reward_curves(writer, i, env, models, models_anotations, tmax, previous_rewards, possible_s_stars, s0_list, estimation_episodes_num)\n",
    "        #fig.tight_layout()\n",
    "        #plt.savefig(os.path.join(plots_dir, \"rewards_curves{}\".format(each_goal_times * (start + i + 1))))\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "        #actions_prob_plot\n",
    "        sns.set(color_codes=True)\n",
    "        fig, ax = subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "        s_stars = [0, env.nS - 1, None]\n",
    "        colors = sns.color_palette(\"Set1\", n_colors=3, desat=.75)\n",
    "        for j, s_star in enumerate(s_stars):\n",
    "            for s in range(1, env.nS - 1):\n",
    "                \n",
    "                if env.grid_map[s] == 0:\n",
    "                    continue\n",
    "                \n",
    "                if s_star is None:\n",
    "                    agent = agent_simple\n",
    "                \n",
    "                else:\n",
    "                    agent = agent_goal_based\n",
    "                \n",
    "                \n",
    "                ax[j].set_xticks(np.arange(0, int(np.sqrt(env.nS)) + 1))\n",
    "                ax[j].set_yticks(np.arange(0, int(np.sqrt(env.nS)) + 1))\n",
    "                probs, state_value = agent(s, s_star)\n",
    "                draw_direction_probs(ax[j], env, s, probs[0].data, arrow_color=colors[j])\n",
    "                if s_star is None:\n",
    "                    ax[j].set_title(agent.name, fontsize=16)\n",
    "                else:\n",
    "                    ax[j].set_title(agent.name + \", s_star={}\".format(str(s_star)), fontsize=16)\n",
    "                ax[j].invert_yaxis()\n",
    "\n",
    "        fig.tight_layout()\n",
    "        img = fig2img(fig)\n",
    "        writer.add_image('Action_Probs', np.array(img)[..., :3], i)\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "\n",
    "        writer.add_scalars('loss terms',\n",
    "                           {\n",
    "                                \"entropy term\": losses_history_goal_based[\"entropy\"][-1],\n",
    "                                \"value term\": losses_history_goal_based[\"value loss\"][-1],\n",
    "                                \"log loss term\": losses_history_goal_based[\"log loss\"][-1],\n",
    "                           }, \n",
    "                           i)\n",
    "\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "\n",
    "        fig, ax = subplots(nrows=1, ncols=3, figsize=(12, 5))\n",
    "        for j in range(3):\n",
    "            ax[j].set_xticks(np.arange(0, int(np.sqrt(env.nS)) + 1))\n",
    "            ax[j].set_yticks(np.arange(0, int(np.sqrt(env.nS)) + 1))\n",
    "            ax[j].invert_yaxis()\n",
    "\n",
    "        for j, s_star in enumerate([0, env.nS - 1, None]):\n",
    "            if s_star is None:\n",
    "                agent = agent_simple\n",
    "            else:\n",
    "                agent = agent_goal_based\n",
    "            #print(agent.name)\n",
    "            state_dist, state_dist_list = play_n_episodes(estimation_episodes_num, env, agent, s0_list, s_star)   \n",
    "            for s in range(1, env.nS - 1):\n",
    "                if env.grid_map[s] == 0:\n",
    "                    continue\n",
    "                \n",
    "                value = agent(s, s_star)[1].data[0][0]\n",
    "                #states_heat_map[int(s // np.sqrt(env.nS))][s % int(np.sqrt(env.nS))] = value\n",
    "                draw_value_anotate(ax[j], env, s, value)\n",
    "            ax[j].imshow(state_dist, cmap='hot', interpolation='nearest')\n",
    "            if s_star is None:\n",
    "                ax[j].set_title(agent.name, fontsize=16)\n",
    "            else:\n",
    "                ax[j].set_title(agent.name + \", s_star={}\".format(str(s_star)), fontsize=16)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        img = fig2img(fig)\n",
    "        writer.add_image('Value state distr', np.array(img)[..., :3], i)\n",
    "\n",
    "        #plt.savefig(os.path.join(plots_dir, \"value_state_distr{}\".format(each_goal_times * (start + i + 1))))\n",
    "        plt.cla()\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "for s in range(1, env.nS - 1):\n",
    "    if env.grid_map[s] == 0:\n",
    "        continue\n",
    "\n",
    "    agent = Expert_global_optimal\n",
    "\n",
    "\n",
    "    ax.set_xticks(np.arange(0, int(np.sqrt(env.nS)) + 1))\n",
    "    ax.set_yticks(np.arange(0, int(np.sqrt(env.nS)) + 1))\n",
    "    probs = agent.action_probs[s]\n",
    "    draw_direction_probs(plt, env, s, probs)\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0. 0. 0. 0. 1.]\n",
      "1 [0. 0. 0. 1. 0.]\n",
      "2 [0. 0. 0. 1. 0.]\n",
      "3 [0. 0. 0. 1. 0.]\n",
      "4 [0. 0. 0. 1. 0.]\n",
      "5 [0. 0. 0. 1. 0.]\n",
      "6 [0. 0. 0. 1. 0.]\n",
      "7 [0. 0. 0. 1. 0.]\n",
      "8 [0. 0. 0. 1. 0.]\n",
      "9 [1. 0. 0. 0. 0.]\n",
      "10 [0.5 0.  0.  0.5 0. ]\n",
      "11 [0.5 0.  0.  0.5 0. ]\n",
      "12 [0.5 0.  0.  0.5 0. ]\n",
      "13 [0.5 0.  0.  0.5 0. ]\n",
      "14 [0.5 0.  0.  0.5 0. ]\n",
      "15 [0.5 0.  0.  0.5 0. ]\n",
      "16 [0.5 0.  0.  0.5 0. ]\n",
      "17 [0. 0. 1. 0. 0.]\n",
      "18 [1. 0. 0. 0. 0.]\n",
      "19 [0.5 0.  0.  0.5 0. ]\n",
      "20 [0.5 0.  0.  0.5 0. ]\n",
      "21 [0.5 0.  0.  0.5 0. ]\n",
      "22 [0.5 0.  0.  0.5 0. ]\n",
      "23 [0.5 0.  0.  0.5 0. ]\n",
      "24 [0.5 0.  0.  0.5 0. ]\n",
      "25 [0.  0.5 0.5 0.  0. ]\n",
      "26 [0. 0. 1. 0. 0.]\n",
      "27 [1. 0. 0. 0. 0.]\n",
      "28 [0.5 0.  0.  0.5 0. ]\n",
      "29 [0.5 0.  0.  0.5 0. ]\n",
      "30 [0.5 0.  0.  0.5 0. ]\n",
      "31 [0.5 0.  0.  0.5 0. ]\n",
      "32 [0.5 0.  0.  0.5 0. ]\n",
      "33 [0.  0.5 0.5 0.  0. ]\n",
      "34 [0.  0.5 0.5 0.  0. ]\n",
      "35 [0. 0. 1. 0. 0.]\n",
      "36 [1. 0. 0. 0. 0.]\n",
      "37 [0.5 0.  0.  0.5 0. ]\n",
      "38 [0.5 0.  0.  0.5 0. ]\n",
      "39 [0.5 0.  0.  0.5 0. ]\n",
      "40 [0.5 0.  0.  0.5 0. ]\n",
      "41 [0.  0.5 0.5 0.  0. ]\n",
      "42 [0.  0.5 0.5 0.  0. ]\n",
      "43 [0.  0.5 0.5 0.  0. ]\n",
      "44 [0. 0. 1. 0. 0.]\n",
      "45 [1. 0. 0. 0. 0.]\n",
      "46 [0.5 0.  0.  0.5 0. ]\n",
      "47 [0.5 0.  0.  0.5 0. ]\n",
      "48 [0.5 0.  0.  0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "for i in range(49):\n",
    "    print(i, Expert_global_optimal.action_probs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "old_net = sia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-04 *\n",
       "  1.0471\n",
       "[torch.FloatTensor of size 1x1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = 0\n",
    "g2 = 1\n",
    "siamese_net(np.array([(g1, g2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.load(\"pretrained_nets/siamese_5x5.pt\")(np.array([(g1, g2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# board.wait(board_timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-11.03"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(get_policy_reward_estimation(env, agent_goal_based, 'agent', 1000, 40, s_star=[0, 24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.2266  0.1318  0.0942  0.5474\n",
       " [torch.FloatTensor of size 1x4], Variable containing:\n",
       " -2.5737\n",
       " [torch.FloatTensor of size 1x1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 1\n",
    "agent_goal_based(s, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.0607  0.4443  0.4541  0.0409\n",
       " [torch.FloatTensor of size 1x4], Variable containing:\n",
       " -0.9203\n",
       " [torch.FloatTensor of size 1x1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_simple(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(get_policy_reward_estimation(env, agent_simple, 'agent', 1000, 40, s_star=[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_simple(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = Variable(torch.FloatTensor([0.5, 0.5]))\n",
    "m = torch.distributions.Categorical(probs)\n",
    "m.log_prob(Variable(torch.LongTensor(1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# import seaborn as sns\n",
    "# fig, ax = subplots(nrows=1, ncols=3, figsize=(12, 5))\n",
    "# for i in range(3):\n",
    "#     ax[i].set_xticks(np.arange(0, 6))\n",
    "#     ax[i].set_yticks(np.arange(0, 6))\n",
    "#     ax[i].invert_yaxis()\n",
    "\n",
    "# for i, s_star in enumerate([0, 24, None]):\n",
    "#     state_dist, state_dist_list = play_n_episodes(100, env, model_Bob, s_star)   \n",
    "#     for s in range(env.nS):\n",
    "#         value = model_Bob(s, s_star)[1].data[0][0]\n",
    "#         #states_heat_map[int(s // np.sqrt(env.nS))][s % int(np.sqrt(env.nS))] = value\n",
    "#         draw_value_anotate(ax[i], env, s, value)\n",
    "#     ax[i].imshow(state_dist, cmap='hot', interpolation='nearest')\n",
    "# #ax[0].colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "c81ce457d2e741af8578f4f3efad1664": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

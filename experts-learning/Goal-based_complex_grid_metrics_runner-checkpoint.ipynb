{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tnrange\n",
    "from random import choice\n",
    "\n",
    "from complex_gridword import GridworldEnv\n",
    "from nets import *\n",
    "from draw_methods import *\n",
    "from hyper_parametrs import *\n",
    "from env_methods import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import command\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shutil.rmtree(logs_directory)\n",
    "writer = SummaryWriter(logs_directory)\n",
    "board = command.Command('tensorboard --logdir=run1:{} --port {}'.format(logs_directory, board_port))\n",
    "board.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./plots/pillar_9x9_siamese/hyper_parametrs.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(\"hyper_parametrs.py\", logs_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/anaconda/lib/python3.5/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['choice']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_size = (9, 9)\n",
    "env_name = \"simple\"\n",
    "grid_map = np.load(\"gridworlds_data/{}_{}x{}/grid_map.npy\".format(env_name, grid_size[0], grid_size[1]))\n",
    "env = GridworldEnv(grid_size, grid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_dim = int(env.nS * 2)\n",
    "action_dim = int(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_term(probs):\n",
    "    return -torch.sum(probs * torch.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params(agent, optimizer, losses_history):\n",
    "    R = 0\n",
    "    saved_actions = agent.saved_actions\n",
    "    value_loss = 0\n",
    "    rewards = []\n",
    "    for r in agent.rewards[::-1]:\n",
    "        R = r + gamma_rl * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    log_loss = Variable(torch.FloatTensor([0]))\n",
    "    val_loss = Variable(torch.FloatTensor([0]))\n",
    "    entropy_loss = Variable(torch.FloatTensor([0]))\n",
    "    for (action, probs, value), r in zip(saved_actions, rewards):\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        reward = r - value.data[0, 0]\n",
    "        log_loss  += -(m.log_prob(action[0]) * reward)\n",
    "        val_loss += lambda_baseline * F.mse_loss(value, Variable(torch.Tensor([r])))\n",
    "        entropy_loss += -entropy_weights[\"agent\"] * entropy_term(probs)\n",
    "    \n",
    "    losses_history[\"entropy\"].append(entropy_loss.data.numpy()[0])\n",
    "    losses_history[\"value loss\"].append(val_loss.data.numpy()[0])\n",
    "    losses_history[\"log loss\"].append(log_loss.data.numpy()[0])\n",
    "    loss = log_loss + val_loss + entropy_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del agent.rewards[:]\n",
    "    del agent.saved_actions[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_type = \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Expert_left_corner = NovelExpert(\"left_top_corner\", np.load(\"gridworlds_data/{}_{}x{}/agents_left_corner/action_probs.npy\".format(env_name, grid_size[0], grid_size[1])), \n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_left_corner/value_function.npy\".format(env_name, grid_size[0], grid_size[1])),\n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_left_corner/goal_map.npy\".format(env_name, grid_size[0], grid_size[1])))\n",
    "Expert_right_corner = NovelExpert(\"right_bottom_corner\", np.load(\"gridworlds_data/{}_{}x{}/agents_right_corner/action_probs.npy\".format(env_name, grid_size[0], grid_size[1])), \n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_right_corner/value_function.npy\".format(env_name, grid_size[0], grid_size[1])),\n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_right_corner/goal_map.npy\".format(env_name, grid_size[0], grid_size[1])))\n",
    "Expert_global_optimal = NovelExpert(\"global_optimal\", np.load(\"gridworlds_data/{}_{}x{}/agents_global_optimal/action_probs.npy\".format(env_name, grid_size[0], grid_size[1])), \n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_global_optimal/value_function.npy\".format(env_name, grid_size[0], grid_size[1])),\n",
    "                            np.load(\"gridworlds_data/{}_{}x{}/agents_global_optimal/goal_map.npy\".format(env_name, grid_size[0], grid_size[1])))\n",
    "possible_s_stars = [0, env.nS - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siamese_net = torch.load(\"pretrained_nets/siamese_{}_{}x{}.pt\".format(env_name, grid_size[0], grid_size[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_true_d = 0 \n",
    "\n",
    "for s1 in range(env.nS):\n",
    "    for s2 in range(env.nS):\n",
    "        if env.grid_map[s1] and env.grid_map[s2]:\n",
    "            max_true_d = max(max_true_d, abs(s1 % int(np.sqrt(env.nS)) - s2 % int(np.sqrt(env.nS))) + abs(s1 // int(np.sqrt(env.nS)) - s2 // int(np.sqrt(env.nS))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def goal_unsim(env, g1, g2, metric_type='siamnet'):\n",
    "    #print(g1)\n",
    "    #print(g2)\n",
    "    if metric_type =='siam':\n",
    "        return (1.0 - siamese_net(np.array([(g1, g2)]))).data.numpy()[0][0]\n",
    "    else:\n",
    "        global env_type\n",
    "        if env.grid_map[g1] == 0 or env.grid_map[g2] == 0:\n",
    "                return 1000\n",
    "        if env_type == \"simple\" or \"pillar\":\n",
    "            return (abs(g1 % int(np.sqrt(env.nS)) - g2 % int(np.sqrt(env.nS))) + abs(g1 // int(np.sqrt(env.nS)) - g2 // int(np.sqrt(env.nS)))) /  max_true_d\n",
    "        else:\n",
    "            if env.grid_map[g1] == 0 or env.grid_map[g2] == 0:\n",
    "                return 1000\n",
    "            r1 = np.abs(Expert_left_corner.v_function[g1] - Expert_left_corner.v_function[g2])\n",
    "            r2 = np.abs(Expert_right_corner.v_function[g1] - Expert_right_corner.v_function[g2])\n",
    "            return min(r1, r2) /  max_true_d\n",
    "    \n",
    "    \n",
    "def goal_based_training(env, tmax, agent, goal, optimizer, losses_history, agent_type): \n",
    "    time = 0\n",
    "    done = False\n",
    "    while True:\n",
    "        if is_terminal(env, env.s): #done\n",
    "            break\n",
    "        time += 1\n",
    "        a = agent.select_action(env.s, goal)\n",
    "        state, _, done, _= env.step(a[0, 0])\n",
    "        if is_terminal(env, env.s):\n",
    "            if is_terminal(env, env.s): #done\n",
    "                break\n",
    "        if time >= tmax:\n",
    "            break\n",
    "        agent.rewards.append(0)\n",
    "    \n",
    "    if \"goal-based\" in agent_type:\n",
    "        final_reward = (-time - lambda_goals  * goal_unsim(env, goal, env.s, agent.metric_type)) / scale_reward \n",
    "    else:\n",
    "        final_reward = (-time) / scale_reward \n",
    "    agent.rewards.append(final_reward)\n",
    "    if(len(agent.saved_actions)):\n",
    "        update_params(agent, optimizer, losses_history)\n",
    "    \n",
    "    del agent.rewards[:]\n",
    "    del agent.saved_actions[:]\n",
    "\n",
    "    return final_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def choose_goal(env, experts, s0, iteration, agent_type=\"goal-based\"):\n",
    "    #print(s0)\n",
    "    if \"goal-based\" in agent_type:\n",
    "        best_goal = None\n",
    "        best_goal_v = -np.inf\n",
    "        possible_goals = [0, env.nS - 1]\n",
    "        for expert in experts:\n",
    "            possible_goals.append(expert.get_goal(s0))\n",
    "            if expert.v_function[s0] > best_goal_v:\n",
    "                best_goal = expert.get_goal(s0)\n",
    "                best_goal_v = expert.v_function[s0]\n",
    "        \n",
    "        if goal_sampling_strategy == \"eps_greedy\":\n",
    "            if np.random.random() < goal_eps:\n",
    "                goal_eps *= goal_eps_decay\n",
    "                return np.random.choice(possible_goals) \n",
    "            else:\n",
    "                return int(best_goal)\n",
    "        \n",
    "        if goal_sampling_strategy == \"random\":\n",
    "            return int(possible_goals[iteration % 2])\n",
    "    return None\n",
    "    \n",
    "def train_agent(env, tmax, each_goal_times, agent, experts, optimizer, losses_history, i, agent_type):\n",
    "    rewards = []\n",
    "    episode_time = []\n",
    "    finish_states = []\n",
    "    set_random_s0(env)\n",
    "    s0 = env.reset()\n",
    "    goal = choose_goal(env, experts, s0, i, agent_type)\n",
    "    res = goal_based_training(env, tmax, agent, goal, optimizer, losses_history, agent_type)   \n",
    "    rewards.append(res)\n",
    "    \n",
    "    return agent, np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_env_s0(env, s0):\n",
    "    env.isd = np.zeros(env.shape[0] * env.shape[1])\n",
    "    env.isd[s0] = 1 \n",
    "\n",
    "def play_n_episodes(n, env, model, s0_list, s_star, tmax=40):\n",
    "    state_dist_list = []\n",
    "    for i in range(n):\n",
    "        set_env_s0(env, s0_list[i])\n",
    "        env.reset()\n",
    "        state_dist_episode = np.zeros(env.shape)\n",
    "        for j in range(tmax):\n",
    "            s = env.s\n",
    "            state_dist_episode[s // env.shape[0]][s % env.shape[1]] += 1\n",
    "            if(is_terminal(env, env.s)):\n",
    "                break\n",
    "            probs, state_value = model(s, s_star)\n",
    "            action = probs.multinomial().data\n",
    "            env.step(action[0][0])\n",
    "        state_dist_list.append(state_dist_episode)\n",
    "    \n",
    "    state_dist = np.zeros(env.shape)\n",
    "    \n",
    "    for dist in state_dist_list:\n",
    "        state_dist += dist\n",
    "    \n",
    "    return state_dist / n, state_dist_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "plots_dir = \"plots_compare\"\n",
    "try:\n",
    "    shutil.rmtree(plots_dir)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(plots_dir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses_history_goal_based = {\n",
    "    \"entropy\":[],\n",
    "    \"value loss\":[],\n",
    "    \"log loss\":[]\n",
    "}\n",
    "\n",
    "losses_history_simple = {\n",
    "    \"entropy\":[],\n",
    "    \"value loss\":[],\n",
    "    \"log loss\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent_goal_based_true = Agent(state_dim, action_dim, \"goal-based\", \"true\")\n",
    "agent_goal_based_siam = Agent(state_dim, action_dim, \"goal-based\", \"siam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g1 = 0\n",
    "g2 = 40\n",
    "goal_unsim(env, g1, g2, metric_type='siamnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/andrejklimkin/anaconda/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/andrejklimkin/anaconda/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Users/andrejklimkin/anaconda/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 goal based last: -5.3240\n",
      "0 goal based mean: -5.3240\n",
      "0 simple agent last: -8.1220\n",
      "0 simple agent mean: -8.1220\n",
      "####################################################################################################\n",
      "\n",
      "1 goal based last: -5.3940\n",
      "1 goal based mean: -5.3590\n",
      "1 simple agent last: -7.7860\n",
      "1 simple agent mean: -7.9540\n",
      "####################################################################################################\n",
      "\n",
      "2 goal based last: -7.8720\n",
      "2 goal based mean: -6.1967\n",
      "2 simple agent last: -7.9920\n",
      "2 simple agent mean: -7.9667\n",
      "####################################################################################################\n",
      "\n",
      "3 goal based last: -5.3000\n",
      "3 goal based mean: -5.9725\n",
      "3 simple agent last: -8.1040\n",
      "3 simple agent mean: -8.0010\n",
      "####################################################################################################\n",
      "\n",
      "4 goal based last: -5.2840\n",
      "4 goal based mean: -5.8348\n",
      "4 simple agent last: -7.9380\n",
      "4 simple agent mean: -7.9884\n",
      "####################################################################################################\n",
      "\n",
      "5 goal based last: -5.2800\n",
      "5 goal based mean: -5.7423\n",
      "5 simple agent last: -7.6640\n",
      "5 simple agent mean: -7.9343\n",
      "####################################################################################################\n",
      "\n",
      "6 goal based last: -5.2440\n",
      "6 goal based mean: -5.6711\n",
      "6 simple agent last: -7.6620\n",
      "6 simple agent mean: -7.8954\n",
      "####################################################################################################\n",
      "\n",
      "7 goal based last: -5.6620\n",
      "7 goal based mean: -5.6700\n",
      "7 simple agent last: -7.8500\n",
      "7 simple agent mean: -7.8897\n",
      "####################################################################################################\n",
      "\n",
      "8 goal based last: -5.2540\n",
      "8 goal based mean: -5.6238\n",
      "8 simple agent last: -7.7080\n",
      "8 simple agent mean: -7.8696\n",
      "####################################################################################################\n",
      "\n",
      "9 goal based last: -5.0880\n",
      "9 goal based mean: -5.5702\n",
      "9 simple agent last: -8.0580\n",
      "9 simple agent mean: -7.8884\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "stats = defaultdict(list)\n",
    "\n",
    "reset_times = 10\n",
    "\n",
    "for time in range(reset_times):\n",
    "    agent_goal_based_true = Agent(state_dim, action_dim, \"goal-based\", \"true\")\n",
    "    agent_goal_based_siam = Agent(state_dim, action_dim, \"goal-based\", \"siam\")\n",
    "    agent_simple = Agent(state_dim, action_dim, \"simple\")\n",
    "\n",
    "    optimizer_goal_based_agent_true = optim.Adam(agent_goal_based.parameters(),lr=lr_agent, weight_decay=weight_decay)\n",
    "    optimizer_goal_based_agent_siam = optim.Adam(agent_goal_based.parameters(),lr=lr_agent, weight_decay=weight_decay)\n",
    "    optimizer_simple_agent = optim.Adam(agent_simple.parameters(),lr=lr_agent, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "    each_goal_times = 1\n",
    "    start = 0\n",
    "    models = [agent_goal_based_true, agent_goal_based_siam, agent_simple, Expert_global_optimal]\n",
    "    experts = [Expert_global_optimal]\n",
    "    reward_history = defaultdict(list)\n",
    "\n",
    "    estimate_every = 500\n",
    "    estimation_episodes_num = 500\n",
    "\n",
    "    for i in tnrange(train_steps):\n",
    "        agent_goal_based_true, rewards = train_agent(env, tmax, each_goal_times, agent_goal_based_true, experts, optimizer_goal_based_agent, losses_history_goal_based_true, i, agent_type=\"goal-based\")\n",
    "        agent_goal_based_siam, rewards = train_agent(env, tmax, each_goal_times, agent_goal_based_siam, experts, optimizer_goal_based_siam, losses_history_goal_based_siam, i, agent_type=\"goal-based\")\n",
    "        agent_simple, rewards = train_agent(env, tmax, each_goal_times, agent_simple, experts, optimizer_simple_agent, losses_history_simple, i, agent_type=\"simple\")\n",
    "\n",
    "        #fig, ax = subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "        models = [agent_goal_based_true, agent_goal_based_siam, agent_simple, Expert_global_optimal]\n",
    "        models_anotations = [\"goal-based true-metric agent\", \"goal-based siam-metric agent\", \"simple agent\", \"Expert global optimal\"]\n",
    "\n",
    "\n",
    "        if (i + 1) % estimate_every == 0:\n",
    "\n",
    "            s0_list = np.random.choice(np.array(np.where(env.grid_map)).flatten(), size=estimation_episodes_num)\n",
    "\n",
    "            current_rewards = get_agents_reward(env, models, models_anotations, tmax, possible_s_stars, s0_list, estimation_episodes_num)\n",
    "\n",
    "            for model_name, model_reward in zip(current_rewards.keys(), current_rewards.values()):\n",
    "                reward_history[model_name].append(np.mean(model_reward))\n",
    "    \n",
    "    stats[\"goal based\"].append(reward_history[\"goal-based agent\"])\n",
    "    print(\"%d goal based last: %.4f\" % (time, np.array(stats[\"goal based\"])[-1, -1]))\n",
    "    print(\"%d goal based mean: %.4f\" % (time, np.mean(np.array(stats[\"goal based\"])[:, -1], axis=0)))\n",
    "    \n",
    "    stats[\"simple agent\"].append(reward_history[\"simple agent\"])\n",
    "    print(\"%d simple agent last: %.4f\" % (time, np.array(stats[\"simple agent\"])[-1, -1]))\n",
    "    print(\"%d simple agent mean: %.4f\" % (time, np.mean(np.array(stats[\"simple agent\"])[:, -1], axis=0)))\n",
    "    \n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-33.8546, -30.5522, -19.695 , -15.7532, -13.9772, -12.9772,\n",
       "       -12.4752, -11.902 , -11.5734, -11.2158, -10.9202, -10.2894,\n",
       "        -9.6354,  -9.4364,  -8.9554,  -9.056 ,  -8.5738,  -8.4132,\n",
       "        -7.9822,  -7.8764,  -7.2082,  -7.1242,  -6.6822,  -6.3096,\n",
       "        -6.347 ,  -6.105 ,  -5.9102,  -5.6952,  -5.6026,  -5.5702])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(stats[\"goal based\"]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(stats).to_csv(\"stats/simple_9x9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12525e668>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAFoCAYAAAA//nPjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEllBgdJklRmcJAkSWUGB0mSVPYJTto0ckkgnekAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12525ed30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.mean(np.array(stats[\"goal based\"]), axis=0))\n",
    "plt.plot(np.mean(np.array(stats[\"simple agent\"]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-13.3784,  -5.9348,  -4.8472,  -4.1542,  -3.9472,  -3.845 ,\n",
       "        -3.7452,  -3.7122,  -3.741 ,  -3.628 ,  -3.6458,  -3.6514,\n",
       "        -3.5716,  -3.4908,  -3.4798,  -3.3726,  -3.3496,  -3.1946,\n",
       "        -3.179 ,  -3.188 ,  -3.11  ,  -3.1232,  -3.0876,  -3.0212,\n",
       "        -3.0388])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(stats[\"simple agent\"]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.666,  -5.924,  -4.728,  -4.102,  -3.952,  -3.962,  -3.818,\n",
       "         -3.646,  -3.89 ,  -3.7  ,  -3.696,  -3.704,  -3.784,  -3.626,\n",
       "         -3.754,  -3.666,  -3.622,  -3.55 ,  -3.63 ,  -3.54 ,  -3.268,\n",
       "         -3.572,  -3.4  ,  -3.456,  -3.504],\n",
       "       [-15.234,  -5.644,  -4.578,  -4.062,  -3.764,  -3.852,  -3.672,\n",
       "         -3.84 ,  -3.628,  -3.796,  -3.602,  -3.502,  -3.526,  -3.598,\n",
       "         -3.506,  -3.432,  -3.676,  -3.258,  -3.506,  -3.472,  -3.492,\n",
       "         -3.486,  -3.33 ,  -3.124,  -3.054],\n",
       "       [-19.294,  -5.694,  -4.752,  -4.128,  -3.758,  -3.762,  -3.66 ,\n",
       "         -3.834,  -3.558,  -3.802,  -3.584,  -3.542,  -3.174,  -3.206,\n",
       "         -3.324,  -3.052,  -2.908,  -2.592,  -2.706,  -2.484,  -2.646,\n",
       "         -2.57 ,  -2.534,  -2.628,  -2.68 ],\n",
       "       [-20.016,  -7.03 ,  -5.164,  -4.056,  -3.584,  -3.014,  -2.938,\n",
       "         -2.732,  -2.626,  -2.44 ,  -2.458,  -2.36 ,  -2.43 ,  -2.428,\n",
       "         -2.342,  -2.454,  -2.464,  -2.384,  -2.46 ,  -2.46 ,  -2.412,\n",
       "         -2.424,  -2.434,  -2.406,  -2.362],\n",
       "       [-18.292,  -5.594,  -4.768,  -4.16 ,  -3.794,  -3.932,  -3.604,\n",
       "         -3.566,  -3.61 ,  -3.522,  -3.518,  -3.46 ,  -3.428,  -3.282,\n",
       "         -3.354,  -3.234,  -3.332,  -2.63 ,  -2.526,  -2.518,  -2.502,\n",
       "         -2.398,  -2.408,  -2.446,  -2.44 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(stats[\"goal based\"]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "for s in range(1, env.nS - 1):\n",
    "    if env.grid_map[s] == 0:\n",
    "        continue\n",
    "\n",
    "    agent = Expert_global_optimal\n",
    "\n",
    "\n",
    "    ax.set_xticks(np.arange(0, int(np.sqrt(env.nS)) + 1))\n",
    "    ax.set_yticks(np.arange(0, int(np.sqrt(env.nS)) + 1))\n",
    "    probs = agent.action_probs[s]\n",
    "    draw_direction_probs(plt, env, s, probs)\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(49):\n",
    "    print(i, Expert_global_optimal.action_probs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "old_net = sia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-04 *\n",
       "  1.0471\n",
       "[torch.FloatTensor of size 1x1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = 0\n",
    "g2 = 1\n",
    "siamese_net(np.array([(g1, g2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.load(\"pretrained_nets/siamese_5x5.pt\")(np.array([(g1, g2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# board.wait(board_timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-11.03"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(get_policy_reward_estimation(env, agent_goal_based, 'agent', 1000, 40, s_star=[0, 24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.2266  0.1318  0.0942  0.5474\n",
       " [torch.FloatTensor of size 1x4], Variable containing:\n",
       " -2.5737\n",
       " [torch.FloatTensor of size 1x1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 1\n",
    "agent_goal_based(s, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejklimkin/Documents/Coursework/2017-2018/main/source/experts-learning/nets.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores), state_values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.0607  0.4443  0.4541  0.0409\n",
       " [torch.FloatTensor of size 1x4], Variable containing:\n",
       " -0.9203\n",
       " [torch.FloatTensor of size 1x1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_simple(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(get_policy_reward_estimation(env, agent_simple, 'agent', 1000, 40, s_star=[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_simple(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = Variable(torch.FloatTensor([0.5, 0.5]))\n",
    "m = torch.distributions.Categorical(probs)\n",
    "m.log_prob(Variable(torch.LongTensor(1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# import seaborn as sns\n",
    "# fig, ax = subplots(nrows=1, ncols=3, figsize=(12, 5))\n",
    "# for i in range(3):\n",
    "#     ax[i].set_xticks(np.arange(0, 6))\n",
    "#     ax[i].set_yticks(np.arange(0, 6))\n",
    "#     ax[i].invert_yaxis()\n",
    "\n",
    "# for i, s_star in enumerate([0, 24, None]):\n",
    "#     state_dist, state_dist_list = play_n_episodes(100, env, model_Bob, s_star)   \n",
    "#     for s in range(env.nS):\n",
    "#         value = model_Bob(s, s_star)[1].data[0][0]\n",
    "#         #states_heat_map[int(s // np.sqrt(env.nS))][s % int(np.sqrt(env.nS))] = value\n",
    "#         draw_value_anotate(ax[i], env, s, value)\n",
    "#     ax[i].imshow(state_dist, cmap='hot', interpolation='nearest')\n",
    "# #ax[0].colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
